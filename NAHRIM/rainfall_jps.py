# -*- coding: utf-8 -*-
"""rainfall_JPS

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/116i-x7bwLtbSe8oS5vd7DbCi64OaWMSP
"""

!pip install scrapy

# Commented out IPython magic to ensure Python compatibility.
# %%writefile rainfall_spider.py
# import scrapy
# from urllib.parse import quote
# 
# HEADERS = {"User-Agent": "Mozilla/5.0"}
# 
# STATE_MAP = {
#     "SEL": "Selangor",
#     "NSN": "Negeri Sembilan",
#     "JHR": "Johor",
#     "PHG": "Pahang",
#     "PER": "Perlis",
#     "KED": "Kedah",
#     "PP": "Pulau Pinang",
#     "PRK": "Perak",
#     "WPKL": "Wilayah Persekutuan Kuala Lumpur",
#     "MEL": "Melaka",
#     "TRG": "Terengganu",
#     "KEL": "Kelantan",
#     "WPL": "Wilayah Persekutuan Labuan",
#     "SBH": "Sabah",
#     "SWK": "Sarawak"
# }
# 
# BASE_URL = "https://publicinfobanjir.water.gov.my/rainfalldata/{}"
# 
# 
# class RainfallSpider(scrapy.Spider):
#     name = "rainfall_spider"
#     custom_settings = {"LOG_LEVEL": "INFO"}
# 
#     def start_requests(self):
#         for state_code, state_name in STATE_MAP.items():
#             url = BASE_URL.format(quote(state_name, safe=""))
#             self.logger.info(f"[RAIN] Fetching {state_code} from {url}")
#             yield scrapy.Request(
#                 url=url,
#                 headers=HEADERS,
#                 callback=self.parse_state,
#                 cb_kwargs={"state_code": state_code,"state_name": state_name},
#             )
# 
#     def pick_rainfall_table(self, response, state_code, state_name):
#         """
#         From all <table>, pick the one whose text contains 'Bil.' and 'ID Stesen'.
#         Otherwise fall back to the last table.
#         """
#         tables = response.css("table")
#         if not tables:
#             self.logger.warning(f"[RAIN] {state_code} {state_name}: no <table> elements found")
#             return None
# 
#         for idx, table in enumerate(tables):
#             txt = " ".join(table.css("::text").getall()).strip()
#             if "Bil." in txt and "ID Stesen" in txt:
#                 return table
# 
#         # fallback
#         self.logger.info(f"[RAIN] {state_code} {state_name}: using last table as fallback")
#         return tables[-1]
# 
#     def parse_state(self, response, state_code, state_name):
#         table = self.pick_rainfall_table(response, state_code, state_name)
# 
#         all_rows = table.css("tr")
#         if len(all_rows) < 3:
#             self.logger.warning(
#                 f"Expected at least 3 rows (2 header + data) in table for {state_name}, "
#                 f"but found {len(all_rows)}"
#             )
#             return
# 
#         header_rows = all_rows[0:2]
#         data_rows = all_rows[2:]
# 
#         # first header row
#         top_ths = header_rows[0].css("th, td")
#         top_texts = [
#             " ".join(th.css("::text").getall()).strip() for th in top_ths
#         ]
# 
#         # second header row. usually date columns
#         bottom_ths = header_rows[1].css("th, td")
#         bottom_texts = [
#             " ".join(th.css("::text").getall()).strip() for th in bottom_ths
#         ]
# 
#         base_cols = top_texts[0:5]     # Bil, ID Stesen, Nama Stesen, Daerah, Kemaskini Terakhir
#         date_cols = bottom_texts[:]    # all dates
#         tail_cols = top_texts[-2:]     # Taburan Hujan dari Tengah Malam, Jumlah 1 Jam
# 
#         columns = base_cols + date_cols + tail_cols
#         ncols = len(columns)
# 
#         count=0
#         for tr in data_rows:
#             cells = tr.css("td, th")
#             tds = [" ".join(td.css("::text").getall()).strip() for td in cells]
# 
#             if len(tds) != ncols:
#                 continue
# 
#             item = {
#                 "state_code": state_code,
#                 "state_name": state_name,
#             }
#             for col, val in zip(columns, tds):
#                 item[col] = val
# 
#             count += 1
#             yield item
# 
#         self.logger.info(f"[RAIN] {state_code} {state_name}: scraped {count} rows")
#

!scrapy runspider rainfall_spider.py -O rainfall_trend.json

import pandas as pd

df = pd.read_json("rainfall_trend.json")
print("Total rows:", len(df))
df.head()