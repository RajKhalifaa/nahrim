# -*- coding: utf-8 -*-
"""waterlevel_JPS

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xEwZSPRDMFHJPlWQZU2O97qJRMxtSgkR
"""

!pip install scrapy

# Commented out IPython magic to ensure Python compatibility.
# %%writefile waterlevel_spider.py
# import scrapy
# from urllib.parse import quote
# 
# HEADERS = {"User-Agent": "Mozilla/5.0"}
# 
# STATE_MAP = {
#     "SEL": "Selangor",
#     "NSN": "Negeri Sembilan",
#     "JHR": "Johor",
#     "PHG": "Pahang",
#     "PER": "Perlis",
#     "KED": "Kedah",
#     "PP": "Pulau Pinang",
#     "PRK": "Perak",
#     "WPKL": "Wilayah Persekutuan Kuala Lumpur",
#     "MEL": "Melaka",
#     "TRG": "Terengganu",
#     "KEL": "Kelantan",
#     "WPL": "Wilayah Persekutuan Labuan",
#     "SBH": "Sabah",
#     "SWK": "Sarawak",
# }
# 
# BASE_URL = "https://publicinfobanjir.water.gov.my/waterleveldata/{}"
# 
# 
# class WaterLevelSpider(scrapy.Spider):
#     name = "waterlevel_spider"
#     custom_settings = {"LOG_LEVEL": "INFO"}
# 
#     def start_requests(self):
#         for state_code, state_name in STATE_MAP.items():
#             url = BASE_URL.format(quote(state_name, safe=""))
#             self.logger.info(f"[WATER] Fetching {state_code} ({state_name}) -> {url}")
#             yield scrapy.Request(
#                 url=url,
#                 headers=HEADERS,
#                 callback=self.parse_state,
#                 cb_kwargs={"state_code": state_code, "state_name": state_name}
#             )
# 
#     def parse_state(self, response, state_code, state_name):
#         table = response.css("table#normaltable1")
#         rows = table.css("tbody tr") if table else []
# 
#         if not rows:
#             self.logger.warning(f"[WATER] {state_code} {state_name}: no rows found")
#             return
# 
#         count = 0
#         for tr in rows:
#             cells = tr.css("td")
#             if len(cells) != 12:
#                 continue
# 
#             bil = cells[0].css("::text").get("").strip()
#             if not bil.isdigit():
#                 continue
# 
#             item = {
#                 "state_code": state_code,
#                 "state_name": state_name,
#                 "bil": bil,
#                 "id_stesen": cells[1].css("::text").get("").strip(),
#                 "nama_stesen": cells[2].css("::text").get("").strip(),
#                 "daerah": cells[3].css("::text").get("").strip(),
#                 "lembangan": cells[4].css("::text").get("").strip(),
#                 "sub_lembangan": cells[5].css("::text").get("").strip(),
#                 "kemaskini_terakhir": cells[6].css("::text").get("").strip(),
#                 "aras_air_m": cells[7].css("a::text").get(default="").strip(),
#                 "ambang_normal": cells[8].css("::text").get("").strip(),
#                 "ambang_waspada": cells[9].css("::text").get("").strip(),
#                 "ambang_amaran": cells[10].css("::text").get("").strip(),
#                 "ambang_bahaya": cells[11].css("::text").get("").strip(),
#             }
#             count += 1
#             yield item
# 
#         self.logger.info(f"[WATER] {state_code} {state_name}: scraped {count} rows")
#

!scrapy runspider waterlevel_spider.py -O waterlevel_all_states.json

import pandas as pd

df = pd.read_json("waterlevel_all_states.json")
print("Total rows:", len(df))
df.head()

